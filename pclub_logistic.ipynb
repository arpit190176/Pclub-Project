{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "5            5      116             74              0        0  25.6   \n",
       "6            3       78             50             32       88  31.0   \n",
       "7           10      115              0              0        0  35.3   \n",
       "8            2      197             70             45      543  30.5   \n",
       "9            8      125             96              0        0   0.0   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  \n",
       "5                     0.201   30        0  \n",
       "6                     0.248   26        1  \n",
       "7                     0.134   29        0  \n",
       "8                     0.158   53        1  \n",
       "9                     0.232   54        1  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=pd.read_csv(\"https://raw.githubusercontent.com/mabhay3420/Deep-Into-CNN/master/Datasets/diabetes2.csv\")\n",
    "data1.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data1 = pd.DataFrame(min_max_scaler.fit_transform(data1), columns=data1.columns, index=data1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=data1.to_numpy()\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35294118, 0.74371859, 0.59016393, ..., 0.50074516, 0.23441503,\n",
       "        0.48333333],\n",
       "       [0.05882353, 0.42713568, 0.54098361, ..., 0.39642325, 0.11656704,\n",
       "        0.16666667],\n",
       "       [0.47058824, 0.91959799, 0.52459016, ..., 0.34724292, 0.25362938,\n",
       "        0.18333333],\n",
       "       ...,\n",
       "       [0.29411765, 0.6080402 , 0.59016393, ..., 0.390462  , 0.07130658,\n",
       "        0.15      ],\n",
       "       [0.05882353, 0.63316583, 0.49180328, ..., 0.4485842 , 0.11571307,\n",
       "        0.43333333],\n",
       "       [0.05882353, 0.46733668, 0.57377049, ..., 0.45305514, 0.10119556,\n",
       "        0.03333333]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data2[:,0:8]\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yo=data2[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(data,Yo,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 8), (154, 8), (614,), (154,))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.reshape(614,1);\n",
    "Y_test.reshape(154,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train=X_train.shape[0]\n",
    "m_test=X_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=X_train.T,X_test.T,Y_train.T,Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0\n",
    "\n",
    "    \n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    assert(w.shape == (dim, 1))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(w,b,X,Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoid( w.T @ X + b )                                   \n",
    "    cost = -1 / m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) \n",
    "    \n",
    "    dw = 1/m * X @ (A - Y).T\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    gradients = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return gradients, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "   \n",
    "    \n",
    "    \n",
    "    A = sigmoid(w.T @ X + b) \n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[:, i] = (A[:, i] > 0.5) * 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w,b,X,Y,num_iterations,learning_rate):\n",
    "    costs=[]\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        gradients,cost = gd(w,b,X,Y)\n",
    "        \n",
    "        dw = gradients[\"dw\"]\n",
    "        db = gradients[\"db\"]\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        costs.append(cost)\n",
    "        \n",
    "        \n",
    "        Y_prediction = predict(w - learning_rate * dw,b - learning_rate * db,X)\n",
    "        accuracy = 100 - np.mean(np.abs(Y_prediction - Y)) * 100\n",
    "        print(\"Epoch {} : (Loss : {}, Correctness : {})\".format(i+1,cost,accuracy))\n",
    "        \n",
    "    params = {\"w\":w,\n",
    "            \"b\":b}\n",
    "    gradients = {\"dw\":dw,\n",
    "                \"db\":db}\n",
    "    return params,gradients,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5):\n",
    "   \n",
    "    \n",
    "    w, b = initialize(X_train.shape[0])\n",
    "\n",
    "    \n",
    "    parameters, grads, costs = loss(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "    \n",
    "\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    " \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "  \n",
    "    print(\"Final train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"Final test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : (Loss : 0.6931471805599454, Correctness : 66.28664495114006)\n",
      "Epoch 2 : (Loss : 0.6745359514745399, Correctness : 66.28664495114006)\n",
      "Epoch 3 : (Loss : 0.66400725244327, Correctness : 66.28664495114006)\n",
      "Epoch 4 : (Loss : 0.6578459613915517, Correctness : 66.28664495114006)\n",
      "Epoch 5 : (Loss : 0.6540574321265217, Correctness : 66.28664495114006)\n",
      "Epoch 6 : (Loss : 0.6515704877586947, Correctness : 66.28664495114006)\n",
      "Epoch 7 : (Loss : 0.6498052375289726, Correctness : 66.28664495114006)\n",
      "Epoch 8 : (Loss : 0.6484445689333382, Correctness : 66.28664495114006)\n",
      "Epoch 9 : (Loss : 0.6473133761851677, Correctness : 66.28664495114006)\n",
      "Epoch 10 : (Loss : 0.6463141452279793, Correctness : 66.28664495114006)\n",
      "Epoch 11 : (Loss : 0.6453921868166965, Correctness : 66.28664495114006)\n",
      "Epoch 12 : (Loss : 0.6445166508092535, Correctness : 66.28664495114006)\n",
      "Epoch 13 : (Loss : 0.6436700503693973, Correctness : 66.28664495114006)\n",
      "Epoch 14 : (Loss : 0.6428424313037097, Correctness : 66.28664495114006)\n",
      "Epoch 15 : (Loss : 0.6420281038884948, Correctness : 66.28664495114006)\n",
      "Epoch 16 : (Loss : 0.6412238007622745, Correctness : 66.28664495114006)\n",
      "Epoch 17 : (Loss : 0.6404276340933539, Correctness : 66.28664495114006)\n",
      "Epoch 18 : (Loss : 0.6396385032433228, Correctness : 66.28664495114006)\n",
      "Epoch 19 : (Loss : 0.6388557574579248, Correctness : 66.28664495114006)\n",
      "Epoch 20 : (Loss : 0.63807900341756, Correctness : 66.28664495114006)\n",
      "Epoch 21 : (Loss : 0.6373079952820092, Correctness : 66.28664495114006)\n",
      "Epoch 22 : (Loss : 0.6365425718045873, Correctness : 66.28664495114006)\n",
      "Epoch 23 : (Loss : 0.6357826203415565, Correctness : 66.28664495114006)\n",
      "Epoch 24 : (Loss : 0.6350280562452928, Correctness : 66.28664495114006)\n",
      "Epoch 25 : (Loss : 0.6342788110630987, Correctness : 66.28664495114006)\n",
      "Epoch 26 : (Loss : 0.6335348257786935, Correctness : 66.28664495114006)\n",
      "Epoch 27 : (Loss : 0.6327960469421743, Correctness : 66.28664495114006)\n",
      "Epoch 28 : (Loss : 0.6320624244545947, Correctness : 66.28664495114006)\n",
      "Epoch 29 : (Loss : 0.631333910300245, Correctness : 66.28664495114006)\n",
      "Epoch 30 : (Loss : 0.6306104578215634, Correctness : 66.28664495114006)\n",
      "Epoch 31 : (Loss : 0.6298920213045857, Correctness : 66.28664495114006)\n",
      "Epoch 32 : (Loss : 0.6291785557419654, Correctness : 66.28664495114006)\n",
      "Epoch 33 : (Loss : 0.628470016697427, Correctness : 66.28664495114006)\n",
      "Epoch 34 : (Loss : 0.627766360228069, Correctness : 66.28664495114006)\n",
      "Epoch 35 : (Loss : 0.6270675428395953, Correctness : 66.28664495114006)\n",
      "Epoch 36 : (Loss : 0.6263735214602354, Correctness : 66.28664495114006)\n",
      "Epoch 37 : (Loss : 0.625684253425231, Correctness : 66.28664495114006)\n",
      "Epoch 38 : (Loss : 0.624999696467266, Correctness : 66.28664495114006)\n",
      "Epoch 39 : (Loss : 0.6243198087102111, Correctness : 66.28664495114006)\n",
      "Epoch 40 : (Loss : 0.6236445486646984, Correctness : 66.28664495114006)\n",
      "Epoch 41 : (Loss : 0.6229738752246902, Correctness : 66.28664495114006)\n",
      "Epoch 42 : (Loss : 0.6223077476645699, Correctness : 66.28664495114006)\n",
      "Epoch 43 : (Loss : 0.6216461256365003, Correctness : 66.28664495114006)\n",
      "Epoch 44 : (Loss : 0.6209889691679047, Correctness : 66.28664495114006)\n",
      "Epoch 45 : (Loss : 0.6203362386589932, Correctness : 66.28664495114006)\n",
      "Epoch 46 : (Loss : 0.6196878948803004, Correctness : 66.28664495114006)\n",
      "Epoch 47 : (Loss : 0.6190438989702088, Correctness : 66.28664495114006)\n",
      "Epoch 48 : (Loss : 0.6184042124324545, Correctness : 66.28664495114006)\n",
      "Epoch 49 : (Loss : 0.6177687971336068, Correctness : 66.28664495114006)\n",
      "Epoch 50 : (Loss : 0.61713761530053, Correctness : 66.28664495114006)\n",
      "Epoch 51 : (Loss : 0.6165106295178197, Correctness : 66.28664495114006)\n",
      "Epoch 52 : (Loss : 0.6158878027252226, Correctness : 66.28664495114006)\n",
      "Epoch 53 : (Loss : 0.6152690982150375, Correctness : 66.28664495114006)\n",
      "Epoch 54 : (Loss : 0.6146544796295026, Correctness : 66.28664495114006)\n",
      "Epoch 55 : (Loss : 0.6140439109581683, Correctness : 66.28664495114006)\n",
      "Epoch 56 : (Loss : 0.6134373565352601, Correctness : 66.28664495114006)\n",
      "Epoch 57 : (Loss : 0.6128347810370307, Correctness : 66.28664495114006)\n",
      "Epoch 58 : (Loss : 0.6122361494791043, Correctness : 66.28664495114006)\n",
      "Epoch 59 : (Loss : 0.6116414272138142, Correctness : 66.28664495114006)\n",
      "Epoch 60 : (Loss : 0.6110505799275339, Correctness : 66.28664495114006)\n",
      "Epoch 61 : (Loss : 0.610463573638005, Correctness : 66.28664495114006)\n",
      "Epoch 62 : (Loss : 0.6098803746916608, Correctness : 66.28664495114006)\n",
      "Epoch 63 : (Loss : 0.6093009497609477, Correctness : 66.28664495114006)\n",
      "Epoch 64 : (Loss : 0.6087252658416455, Correctness : 66.28664495114006)\n",
      "Epoch 65 : (Loss : 0.6081532902501874, Correctness : 66.28664495114006)\n",
      "Epoch 66 : (Loss : 0.6075849906209793, Correctness : 66.28664495114006)\n",
      "Epoch 67 : (Loss : 0.6070203349037222, Correctness : 66.28664495114006)\n",
      "Epoch 68 : (Loss : 0.6064592913607352, Correctness : 66.28664495114006)\n",
      "Epoch 69 : (Loss : 0.605901828564282, Correctness : 66.28664495114006)\n",
      "Epoch 70 : (Loss : 0.6053479153939006, Correctness : 66.28664495114006)\n",
      "Epoch 71 : (Loss : 0.6047975210337383, Correctness : 66.28664495114006)\n",
      "Epoch 72 : (Loss : 0.6042506149698902, Correctness : 66.28664495114006)\n",
      "Epoch 73 : (Loss : 0.603707166987745, Correctness : 66.28664495114006)\n",
      "Epoch 74 : (Loss : 0.6031671471693356, Correctness : 66.28664495114006)\n",
      "Epoch 75 : (Loss : 0.6026305258906979, Correctness : 66.28664495114006)\n",
      "Epoch 76 : (Loss : 0.6020972738192361, Correctness : 66.44951140065146)\n",
      "Epoch 77 : (Loss : 0.6015673619110968, Correctness : 66.28664495114006)\n",
      "Epoch 78 : (Loss : 0.6010407614085518, Correctness : 66.28664495114006)\n",
      "Epoch 79 : (Loss : 0.6005174438373885, Correctness : 66.44951140065146)\n",
      "Epoch 80 : (Loss : 0.5999973810043123, Correctness : 66.44951140065146)\n",
      "Epoch 81 : (Loss : 0.5994805449943581, Correctness : 66.61237785016286)\n",
      "Epoch 82 : (Loss : 0.5989669081683112, Correctness : 66.61237785016286)\n",
      "Epoch 83 : (Loss : 0.5984564431601407, Correctness : 66.77524429967427)\n",
      "Epoch 84 : (Loss : 0.5979491228744445, Correctness : 66.77524429967427)\n",
      "Epoch 85 : (Loss : 0.5974449204839052, Correctness : 66.77524429967427)\n",
      "Epoch 86 : (Loss : 0.5969438094267582, Correctness : 66.77524429967427)\n",
      "Epoch 87 : (Loss : 0.5964457634042736, Correctness : 66.77524429967427)\n",
      "Epoch 88 : (Loss : 0.5959507563782498, Correctness : 66.77524429967427)\n",
      "Epoch 89 : (Loss : 0.5954587625685213, Correctness : 66.77524429967427)\n",
      "Epoch 90 : (Loss : 0.5949697564504792, Correctness : 66.77524429967427)\n",
      "Epoch 91 : (Loss : 0.5944837127526067, Correctness : 66.77524429967427)\n",
      "Epoch 92 : (Loss : 0.5940006064540276, Correctness : 66.77524429967427)\n",
      "Epoch 93 : (Loss : 0.5935204127820709, Correctness : 66.77524429967427)\n",
      "Epoch 94 : (Loss : 0.5930431072098478, Correctness : 66.77524429967427)\n",
      "Epoch 95 : (Loss : 0.5925686654538465, Correctness : 66.77524429967427)\n",
      "Epoch 96 : (Loss : 0.5920970634715396, Correctness : 66.61237785016286)\n",
      "Epoch 97 : (Loss : 0.5916282774590084, Correctness : 66.77524429967427)\n",
      "Epoch 98 : (Loss : 0.5911622838485833, Correctness : 66.77524429967427)\n",
      "Epoch 99 : (Loss : 0.5906990593064979, Correctness : 67.10097719869708)\n",
      "Epoch 100 : (Loss : 0.5902385807305613, Correctness : 67.58957654723127)\n",
      "Epoch 101 : (Loss : 0.5897808252478457, Correctness : 67.58957654723127)\n",
      "Epoch 102 : (Loss : 0.58932577021239, Correctness : 67.75244299674267)\n",
      "Epoch 103 : (Loss : 0.5888733932029201, Correctness : 67.75244299674267)\n",
      "Epoch 104 : (Loss : 0.5884236720205867, Correctness : 67.75244299674267)\n",
      "Epoch 105 : (Loss : 0.5879765846867177, Correctness : 67.75244299674267)\n",
      "Epoch 106 : (Loss : 0.5875321094405903, Correctness : 67.75244299674267)\n",
      "Epoch 107 : (Loss : 0.5870902247372171, Correctness : 67.75244299674267)\n",
      "Epoch 108 : (Loss : 0.5866509092451516, Correctness : 67.75244299674267)\n",
      "Epoch 109 : (Loss : 0.5862141418443098, Correctness : 67.75244299674267)\n",
      "Epoch 110 : (Loss : 0.5857799016238082, Correctness : 67.75244299674267)\n",
      "Epoch 111 : (Loss : 0.5853481678798211, Correctness : 67.91530944625407)\n",
      "Epoch 112 : (Loss : 0.584918920113453, Correctness : 68.24104234527687)\n",
      "Epoch 113 : (Loss : 0.5844921380286304, Correctness : 68.56677524429968)\n",
      "Epoch 114 : (Loss : 0.584067801530009, Correctness : 68.56677524429968)\n",
      "Epoch 115 : (Loss : 0.5836458907209002, Correctness : 68.72964169381108)\n",
      "Epoch 116 : (Loss : 0.583226385901214, Correctness : 68.56677524429968)\n",
      "Epoch 117 : (Loss : 0.5828092675654192, Correctness : 68.72964169381108)\n",
      "Epoch 118 : (Loss : 0.5823945164005221, Correctness : 68.72964169381108)\n",
      "Epoch 119 : (Loss : 0.5819821132840621, Correctness : 68.89250814332247)\n",
      "Epoch 120 : (Loss : 0.5815720392821246, Correctness : 68.89250814332247)\n",
      "Epoch 121 : (Loss : 0.581164275647372, Correctness : 68.89250814332247)\n",
      "Epoch 122 : (Loss : 0.5807588038170916, Correctness : 69.05537459283387)\n",
      "Epoch 123 : (Loss : 0.5803556054112612, Correctness : 69.21824104234527)\n",
      "Epoch 124 : (Loss : 0.5799546622306327, Correctness : 69.21824104234527)\n",
      "Epoch 125 : (Loss : 0.5795559562548327, Correctness : 69.21824104234527)\n",
      "Epoch 126 : (Loss : 0.5791594696404793, Correctness : 69.21824104234527)\n",
      "Epoch 127 : (Loss : 0.5787651847193187, Correctness : 69.21824104234527)\n",
      "Epoch 128 : (Loss : 0.5783730839963773, Correctness : 69.21824104234527)\n",
      "Epoch 129 : (Loss : 0.5779831501481321, Correctness : 69.05537459283387)\n",
      "Epoch 130 : (Loss : 0.5775953660206967, Correctness : 69.05537459283387)\n",
      "Epoch 131 : (Loss : 0.5772097146280266, Correctness : 69.05537459283387)\n",
      "Epoch 132 : (Loss : 0.5768261791501406, Correctness : 69.05537459283387)\n",
      "Epoch 133 : (Loss : 0.5764447429313582, Correctness : 69.05537459283387)\n",
      "Epoch 134 : (Loss : 0.5760653894785565, Correctness : 69.05537459283387)\n",
      "Epoch 135 : (Loss : 0.5756881024594414, Correctness : 69.05537459283387)\n",
      "Epoch 136 : (Loss : 0.5753128657008368, Correctness : 68.89250814332247)\n",
      "Epoch 137 : (Loss : 0.5749396631869916, Correctness : 68.89250814332247)\n",
      "Epoch 138 : (Loss : 0.5745684790579009, Correctness : 68.89250814332247)\n",
      "Epoch 139 : (Loss : 0.5741992976076461, Correctness : 69.05537459283387)\n",
      "Epoch 140 : (Loss : 0.5738321032827499, Correctness : 69.21824104234527)\n",
      "Epoch 141 : (Loss : 0.5734668806805496, Correctness : 69.21824104234527)\n",
      "Epoch 142 : (Loss : 0.5731036145475841, Correctness : 69.21824104234527)\n",
      "Epoch 143 : (Loss : 0.5727422897779993, Correctness : 69.21824104234527)\n",
      "Epoch 144 : (Loss : 0.5723828914119694, Correctness : 69.21824104234527)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145 : (Loss : 0.572025404634133, Correctness : 69.38110749185668)\n",
      "Epoch 146 : (Loss : 0.5716698147720463, Correctness : 69.21824104234527)\n",
      "Epoch 147 : (Loss : 0.5713161072946518, Correctness : 69.21824104234527)\n",
      "Epoch 148 : (Loss : 0.5709642678107625, Correctness : 69.05537459283387)\n",
      "Epoch 149 : (Loss : 0.5706142820675628, Correctness : 69.05537459283387)\n",
      "Epoch 150 : (Loss : 0.5702661359491232, Correctness : 69.05537459283387)\n",
      "Epoch 151 : (Loss : 0.5699198154749315, Correctness : 69.38110749185668)\n",
      "Epoch 152 : (Loss : 0.56957530679844, Correctness : 69.38110749185668)\n",
      "Epoch 153 : (Loss : 0.5692325962056262, Correctness : 69.38110749185668)\n",
      "Epoch 154 : (Loss : 0.5688916701135699, Correctness : 69.54397394136808)\n",
      "Epoch 155 : (Loss : 0.568552515069045, Correctness : 69.70684039087948)\n",
      "Epoch 156 : (Loss : 0.5682151177471264, Correctness : 69.86970684039088)\n",
      "Epoch 157 : (Loss : 0.5678794649498103, Correctness : 69.86970684039088)\n",
      "Epoch 158 : (Loss : 0.5675455436046515, Correctness : 69.86970684039088)\n",
      "Epoch 159 : (Loss : 0.5672133407634132, Correctness : 70.19543973941367)\n",
      "Epoch 160 : (Loss : 0.5668828436007324, Correctness : 70.35830618892508)\n",
      "Epoch 161 : (Loss : 0.5665540394127988, Correctness : 70.35830618892508)\n",
      "Epoch 162 : (Loss : 0.5662269156160488, Correctness : 70.35830618892508)\n",
      "Epoch 163 : (Loss : 0.5659014597458725, Correctness : 70.35830618892508)\n",
      "Epoch 164 : (Loss : 0.5655776594553362, Correctness : 70.19543973941367)\n",
      "Epoch 165 : (Loss : 0.5652555025139167, Correctness : 70.19543973941367)\n",
      "Epoch 166 : (Loss : 0.5649349768062506, Correctness : 70.19543973941367)\n",
      "Epoch 167 : (Loss : 0.5646160703308978, Correctness : 70.35830618892508)\n",
      "Epoch 168 : (Loss : 0.564298771199117, Correctness : 70.35830618892508)\n",
      "Epoch 169 : (Loss : 0.5639830676336551, Correctness : 70.35830618892508)\n",
      "Epoch 170 : (Loss : 0.5636689479675508, Correctness : 70.35830618892508)\n",
      "Epoch 171 : (Loss : 0.5633564006429496, Correctness : 70.52117263843648)\n",
      "Epoch 172 : (Loss : 0.5630454142099333, Correctness : 70.68403908794788)\n",
      "Epoch 173 : (Loss : 0.5627359773253615, Correctness : 70.68403908794788)\n",
      "Epoch 174 : (Loss : 0.5624280787517268, Correctness : 70.68403908794788)\n",
      "Epoch 175 : (Loss : 0.5621217073560213, Correctness : 70.84690553745928)\n",
      "Epoch 176 : (Loss : 0.5618168521086164, Correctness : 71.00977198697069)\n",
      "Epoch 177 : (Loss : 0.5615135020821559, Correctness : 71.00977198697069)\n",
      "Epoch 178 : (Loss : 0.5612116464504594, Correctness : 71.00977198697069)\n",
      "Epoch 179 : (Loss : 0.5609112744874396, Correctness : 71.00977198697069)\n",
      "Epoch 180 : (Loss : 0.5606123755660306, Correctness : 71.00977198697069)\n",
      "Epoch 181 : (Loss : 0.5603149391571289, Correctness : 70.84690553745928)\n",
      "Epoch 182 : (Loss : 0.5600189548285459, Correctness : 70.84690553745928)\n",
      "Epoch 183 : (Loss : 0.5597244122439717, Correctness : 71.00977198697069)\n",
      "Epoch 184 : (Loss : 0.5594313011619517, Correctness : 71.00977198697069)\n",
      "Epoch 185 : (Loss : 0.5591396114348723, Correctness : 71.00977198697069)\n",
      "Epoch 186 : (Loss : 0.5588493330079619, Correctness : 71.00977198697069)\n",
      "Epoch 187 : (Loss : 0.5585604559182982, Correctness : 71.17263843648209)\n",
      "Epoch 188 : (Loss : 0.5582729702938309, Correctness : 71.17263843648209)\n",
      "Epoch 189 : (Loss : 0.5579868663524139, Correctness : 71.33550488599349)\n",
      "Epoch 190 : (Loss : 0.5577021344008464, Correctness : 71.49837133550488)\n",
      "Epoch 191 : (Loss : 0.5574187648339288, Correctness : 71.49837133550488)\n",
      "Epoch 192 : (Loss : 0.5571367481335255, Correctness : 71.49837133550488)\n",
      "Epoch 193 : (Loss : 0.5568560748676413, Correctness : 71.49837133550488)\n",
      "Epoch 194 : (Loss : 0.556576735689505, Correctness : 71.82410423452768)\n",
      "Epoch 195 : (Loss : 0.5562987213366674, Correctness : 71.82410423452768)\n",
      "Epoch 196 : (Loss : 0.5560220226301054, Correctness : 71.82410423452768)\n",
      "Epoch 197 : (Loss : 0.5557466304733393, Correctness : 71.98697068403908)\n",
      "Epoch 198 : (Loss : 0.5554725358515588, Correctness : 72.14983713355049)\n",
      "Epoch 199 : (Loss : 0.5551997298307586, Correctness : 72.14983713355049)\n",
      "Epoch 200 : (Loss : 0.5549282035568854, Correctness : 72.31270358306189)\n",
      "Final train accuracy: 72.14983713355049 %\n",
      "Final test accuracy: 66.23376623376623 %\n"
     ]
    }
   ],
   "source": [
    "model(X_train, Y_train, X_test, Y_test, num_iterations = 200, learning_rate = 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
